# Covariance and Correlation

In many engineering and scientific applications, there are multiple variables involved. For instance, in structural engineering when assessing the health of a structure, we might have to take into account the different loads on the structure as well as the deterioration of the building materials. In climate science, when trying to study the effect of climate change in agricultural production, we might have to consider the impact of the changes in temperature, soil moisture and precipitation, amongst others, in vegetation. 

These variables of interest are often "tied" to one another. For instance, the different variables to describe the wind loads on a building (e.g.: wind speed and wind direction) are generated by the same drivers and, thus, are dependent on each other. Also, the interest variables might be part of the same physical process: as temperature increases, some of the soil moisture evaporates, which might impact on the precipitation later. 

In order to assess the streght of the dependence in these complex relationships between variables of interest, we can use statistical metrics for dependence. Here, we will discuss _covariance_ and _Pearson's correlation coefficient_.

## Covariance: definition

Covariance is a measure of the joint variability of two variables. The definition of Covariance is given by

$$
\text{Cov}(X_1, X_2) = \mathbb{E}[(X_1 - \mathbb{E}(X_1))(X_2 - \mathbb{E}(X_2))] = \mathbb{E}(X_1 X_2)-\mathbb{E}(X_1)\mathbb{E}(X_2)
$$

The covariance of two random variables can take both positive and negative values and has units equal to the product of the units of the analyzed variables. High absolute values of covariance imply a strong relationship between variables.

If $Cov(X_1,X_2)>0$, high values of $X_1$ typically occur together with high values of $X_2$, while low values of $X_1$ typically occur with low values of $X_2$, therefore the covariance is defined as *POSITIVE*.

On the other hand, if $Cov(X_1,X_2)<0$, high values of $X_1$ typically occur together with low values of $X_2$, while low values of $X_1$ typically occur with high values of $X_2$. In this case the covariance is defined as *NEGATIVE*.

Note that the covariance of one variable with itself is equal to the variance.

$$
Cov(X,X)=\mathbb{E}([X_i-\mathbb{E}(X_i)]^2) =\sigma^2_{X}
$$

## Covariance: geometric interpretation

Imagine that I am studying the relationship between the height and the Body Mass Index ($BMI$) of people. I have a short dataset that it is shown as a scatter plot in the image below.

```{figure} ../figures/obs_geom.png

---

---
Geometric interpretation of the covariance: (a) paired observations of height and BMI, and (b) rectangular areas defined by the observations and the mean values of the two random variables, which represent $X_i-E(X)$.
```

## Covariance matrix

When considering a random vector $X= [\begin{array}{llll} X_1 & X_2 & \ldots &X_m \end{array}]^T$, we can 'collect' all covariances in the so-called *covariance matrix*:

$$
\Sigma_X=  \left[\begin{array}{cccc} \sigma^2_1 & Cov(X_1,X_2) & \ldots & Cov(X_1,X_m) \\ Cov(X_1,X_2)& \sigma_{2}^2 & \ldots & Cov(X_2,X_m) \\\vdots & \vdots & \ddots & \vdots \\ Cov(X_1,X_m) & Cov(X_2,X_m) & \ldots & \sigma_{m}^2 \end{array}\right]
$$

Note that the covariance matrix is symmetric, since $Cov(X_i,X_j)= Cov(X_j,X_i)$. 

If all measurements are independent, all covariances will be equal to zero, and the covariance matrix becomes a diagonal matrix with the variances on the diagonal. 

### Correlation

A simple way to assess statistically whether two variables are related is their (linear) correlation, which describes how change in respect to one another. One of the most popular ways to calculate the correlation is the Pearson correlation r:

$$
\rho = \frac{\sum_{i=1}^{n} (X_{1i} - \bar{X_1})(X_{2i} - \bar{X_2})}{\sqrt{\sum_{i=1}^{n} (X_{1i} - \bar{X_1})^2 \sum_{i=1}^{n} (X_{2i} - \bar{X_2})^2}}
$$

Where:
- $X_{1i}$ and  $X_{2i}$ are the individual data points,
- $\bar{X_1}$ and $\bar{X_2}$ are the means of $X_1$ and $X_2$,
- $n$ is the number of data points.

Pearson correlation is also related to covariance as

$$ 
\rho(X_1,X_2)=\frac{Cov(X_1, X_2)}{\sigma_{X_1} \sigma_{X_2}} 
$$ 

In the plot below you can see and try how the values of $X_1$ and $X_2$ vary together given their correlation. 

<iframe src="../_static/elements/element_correlation.html" width="600" height="400" frameborder="0"></iframe>