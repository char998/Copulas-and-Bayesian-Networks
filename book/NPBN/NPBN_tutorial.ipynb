{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dDcxip5PjAKT"
   },
   "source": [
    "# Tutorial: Non-parametric Bayesian Network with py-Banshee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c-SOw0tljAKV"
   },
   "source": [
    "In this tutorial, we will introduce you to the software Banshee to use non-parametric Bayesian Networks (NPBN). You can find in the links the full information about the last versions of the package implementations in [Matlab](http://dx.doi.org/10.1016/j.softx.2023.101479) and [Python](http://dx.doi.org/10.1016/j.softx.2022.101279). \n",
    "\n",
    "In case of running with issues when installing py-Banshee in your laptop, please, refer to the installation instructions provided in Brightspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "thebe-remove-input-init",
     "auto-execute-page"
    ]
   },
   "outputs": [],
   "source": [
    "import micropip\n",
    "await micropip.install(\"../packages/py_banshee-1.1.1-py3-none-any.whl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "53uXz9eTjAKX"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "\n",
    "from py_banshee.rankcorr import bn_rankcorr\n",
    "from py_banshee.bn_plot import bn_visualize\n",
    "from py_banshee.d_cal import test_distance\n",
    "from py_banshee.copula_test import cvm_statistic\n",
    "from py_banshee.prediction import inference,conditional_margins_hist\n",
    "\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data exploration with py-Banshee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the main assumptions of non-parametric Bayesian Networks is the assumption of the Gaussian copula. In order to validate this hypothesis, two possible tests can be performed: Cramer von Mises statistic, and semicorrelations. \n",
    "\n",
    "Here, we will see how to use the implementation of Cramer von Mises test in py-Banshee for a fast exploration. Note that this is not the only exploration that you should perform when starting working with data. This tutorial is just meant to show you the capabilities within py-Banshee.\n",
    "\n",
    "Let's first load a toy dataset with wave and wind variables for the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('dataset.csv')\n",
    "data.head() #shows the first 5 rows of the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data, height=1,plot_kws=dict(size=.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have imported a DataFrame with 4 columns and 299 rows. Each column corresponds to the axle load of one axle in tones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also compute the Cramer-von_mises statistic to compare the empirical copula with the Gaussian, Frank, Gumbel and Clayton copulas and assess whether Gaussian copula is a reasonable hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings  \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "M = cvm_statistic(data, plot=1, names=data.columns,fig_name='CvM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1cTBstL1jAKZ"
   },
   "source": [
    "### 2. Defining the DAG of the NPBN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will define the following Directed Acyclic Graph (DAG):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "G = nx.DiGraph()\n",
    "G.add_node('0')\n",
    "G.add_node('1')\n",
    "G.add_node('2')\n",
    "G.add_node('3')\n",
    "\n",
    "# Add edges\n",
    "G.add_edge('0', '1')\n",
    "G.add_edge('1', '2')\n",
    "G.add_edge('0', '3')\n",
    "G.add_edge('2', '3')\n",
    "\n",
    "pos = {'0': (0.1, 0.1), '1': (0.1, 0), '2': (0.2, 0), '3': (0.2, 0.1)}\n",
    "\n",
    "colors = ['white', 'white', 'white', 'white']\n",
    "sizes = [2000, 2000, 2000, 2000]\n",
    "\n",
    "\n",
    "# Draw the graph\n",
    "plt.figure(figsize=(5, 5))\n",
    "nx.draw_networkx_nodes(G, pos, node_color=colors, node_size=sizes, edgecolors = 'black')\n",
    "nx.draw_networkx_edges(G, pos, edge_color='black', alpha=0.5, arrows=True, arrowstyle = '-|>', \n",
    "                       arrowsize=50, min_source_margin=0.1, min_target_margin=21)  \n",
    "nx.draw_networkx_labels(G, pos)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define the DAG, we follow the next steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the number of nodes in the DAG\n",
    "N = data.shape[1]\n",
    "\n",
    "#Create a list with length equal to the number of nodes\n",
    "parents1 = [None]*N  # create an empty list \n",
    "\n",
    "#In the created list, we will indicate in the position i the nodes that are parents of i.\n",
    "#Note that the numbering of the nodes in Python starts in 0\n",
    "parents1[0] = []     # AW1 does not have parents       \n",
    "parents1[1] = [0]    # The parent of AW2 is AW1          \n",
    "parents1[2] = [1]    # The parent of AW3 is AW2     \n",
    "parents1[3] = [0, 2]    # The parent of AW4 is AW1 and AW3  \n",
    "\n",
    "#Additionally, we have extracted the names of the variables in the nodes\n",
    "names = list(data.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vvUBmoaKjAKd"
   },
   "source": [
    "Now, we can calculate the rank correlation matrix of the NPBN that we have defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 672
    },
    "id": "Xo1-A_87jAKe",
    "outputId": "b044bef2-879f-4407-b6ec-46e457f5875f"
   },
   "outputs": [],
   "source": [
    "R_BN=bn_rankcorr(parents1,data,var_names=names,is_data=True, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l2H2hdEgjAKf"
   },
   "source": [
    "And we can also display the graph to check whether the defined DAG is the desired one.\n",
    "\n",
    "*Note: This cell may give an error due to the package Graphviz. This package is only used for visualization so don't worry. However, if you want to solve the problem locally in your laptop, you can go to the documentation on Brightspace*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ZXJkLh1cSpAr",
    "outputId": "85e55739-0d9f-43e2-aaae-f222a68b466a"
   },
   "outputs": [],
   "source": [
    "bn_visualize(parents1,                           # structure of the BN\n",
    "             R_BN,                                 # the rank correlation matrix \n",
    "             names,                             # names of variables\n",
    "             data = data,                       # DataFrame with data\n",
    "             fig_name =  'NPBN_1')  # figure name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ec2C49FhjAKg"
   },
   "source": [
    "### 3. Assessing the accuracy of the DAG\n",
    "\n",
    "We can compute the d-calibration score between two rank correlation matrices to compare them. Here we will do it to compare the empirical rank correlation matrix (R_e) and the rank correlation matrix obtained for the BN (R_BN). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 572
    },
    "id": "08iSC0B1jAKg",
    "outputId": "15d0261f-f2e3-4a14-9366-db3c3a0c505f"
   },
   "outputs": [],
   "source": [
    "#Compute the empirical rank correlation matrix using Scipy.stats\n",
    "R_e = stats.spearmanr(data)[0]\n",
    "\n",
    "#Compute the d-calibration score\n",
    "d_cal_E_N = 1 - test_distance(R_e, R_BN, 'H', data.shape[1])\n",
    "print('The d-calibration score between R_e and R_N is', d_cal_E_N.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DqupkxtujAKi"
   },
   "source": [
    "### 4. Using the NPBN\n",
    "\n",
    "We can use the option 'inference' to conditionalize the joint distribution on a value of one of the random variables. Here we will do it for one variable, but same process can be done for more than one variable at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xj1djZ1tjAKi"
   },
   "outputs": [],
   "source": [
    "#First we define the nodes in which we want to conditionalize, where we know the value\n",
    "condition_nodes = [0]\n",
    "\n",
    "#Then we define the value for the variable\n",
    "condition_values = [3]\n",
    "\n",
    "F = inference(Nodes = condition_nodes,\n",
    "            Values = condition_values,\n",
    "            R=R_BN, #we provide the rank correlation matrix that defines the joint distribution\n",
    "            DATA=data, #we provide the dataset\n",
    "            empirical_data=True, #we use empirical margins, we could also use parametric margins\n",
    "            SampleSize=1000, #we define the number of conditional samples we want to define the conditional distribution\n",
    "            Output='full')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VzOsqXAYjAKk"
   },
   "source": [
    "Using these samples, we can then plot the conditional and un-conditional marginals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,3, figsize=(15,6), layout='constrained')\n",
    "axes[0].hist(F[0][0], density = True, label = 'Conditional')\n",
    "axes[0].hist(data.iloc[:,1], density = True, label = 'Unconditional', alpha = 0.5)\n",
    "axes[0].set_title('Tm (s)',fontsize=18)\n",
    "axes[0].legend(fontsize=14)\n",
    "\n",
    "axes[1].hist(F[0][1], density = True, label = 'Conditional')\n",
    "axes[1].hist(data.iloc[:,2], density = True, label = 'Unconditional', alpha = 0.5)\n",
    "axes[1].set_title('Tp (s)',fontsize=18)\n",
    "axes[1].legend(fontsize=14)\n",
    "\n",
    "axes[2].hist(F[0][2], density = True, label = 'Conditional')\n",
    "axes[2].hist(data.iloc[:,3], density = True, label = 'Unconditional', alpha = 0.5)\n",
    "axes[2].set_title('Ws ms-1',fontsize=18)\n",
    "axes[2].legend(fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End of the tutorial."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "interpreter": {
   "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
